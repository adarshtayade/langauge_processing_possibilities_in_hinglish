{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0p_sdV7R8Lj6"
   },
   "outputs": [],
   "source": [
    "# HINDI ANALYSIS TO FIND ENGLISH WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G750Foue8Lj9"
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "import sys,os\n",
    "import numpy as np\n",
    "from nltk.metrics import accuracy\n",
    "\n",
    "import nltk, re, pprint\n",
    "import sys,os\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download()\n",
    "#sudo apt-get install python3-pil.imagetk #installation for tree visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "on9h6w5a8Lj-",
    "outputId": "80697a7c-07dd-4e8d-ef55-cb1917cc54ca",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('लड़कियों', 'NN'),\n",
       " ('को', 'PREP'),\n",
       " ('हथियार', 'NN'),\n",
       " ('चलाना', 'VNN'),\n",
       " (',', 'PUNC'),\n",
       " ('जासूसी', 'NVB'),\n",
       " ('करना', 'VNN'),\n",
       " ('और', 'CC'),\n",
       " ('मानव', 'NNC'),\n",
       " ('बम', 'NN'),\n",
       " ('बन', 'VFM'),\n",
       " ('कर', 'VRB'),\n",
       " ('इस्लाम', 'NNP'),\n",
       " ('के', 'PREP'),\n",
       " ('नाम', 'NN'),\n",
       " ('पर', 'PREP'),\n",
       " ('शहीद', 'JVB'),\n",
       " ('होने', 'VNN'),\n",
       " ('की', 'PREP'),\n",
       " ('ट्रेनिंग', 'NN'),\n",
       " ('खुलेआम', 'RB'),\n",
       " ('दी', 'VFM'),\n",
       " ('जाती', 'VAUX'),\n",
       " ('है', 'VAUX'),\n",
       " ('।', 'PUNC')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()\n",
    "tokens = 'Colorless green ideas sleep furiously .'.split()\n",
    "brown_tagged = nltk.corpus.brown.tagged_words()\n",
    "# Unigram Tagger\n",
    "brown_tagged = nltk.corpus.indian.tagged_sents('hindi.pos')\n",
    "autom = open(r\"C:\\Users\\Lenovo\\ayush\\Project\\Project\\Final\\Hindi-News\\auto_text_final.txt\", encoding=\"utf-8\").read()\n",
    "auto = autom.split()\n",
    "#hind = open(r\"C:\\Users\\Lenovo\\ayush\\Project\\Project\\Final\\Hindi-News\\auto_text_final.txt\", encoding=\"utf-8\").read().split\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(brown_tagged)\n",
    "t2 = nltk.BigramTagger(brown_tagged, backoff=t1)\n",
    "s= \"लड़कियों को हथियार चलाना , जासूसी करना और मानव बम बन कर इस्लाम के नाम पर शहीद होने की ट्रेनिंग खुलेआम दी जाती है ।\".split()\n",
    "#a = list(t2.tag(auto))\n",
    "t2.tag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4YPpFky8Lj_",
    "outputId": "535049fc-7a23-4b8d-fd23-35370e448534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('पूर्ण', 'JJ'), ('प्रतिबंध', 'NN'), ('हटाओ', 'VFM'), (':', 'SYM'), ('इराक', 'NNP')], [('संयुक्त', 'NNC'), ('राष्ट्र', 'NN'), ('।', 'SYM')], ...]\n"
     ]
    }
   ],
   "source": [
    "print(brown_tagged)\n",
    "dict_name={}\n",
    "dict_tags={}\n",
    "with open(\"dictionary.txt\", \"w+\", encoding ='utf-8') as f:\n",
    "    for sentence in brown_tagged :\n",
    "        for (word,tag) in sentence :\n",
    "            if word in dict_name:\n",
    "                dict_name[word] = dict_name[word] + 1\n",
    "                if tag in dict_tags :\n",
    "                    dict_tags[tag] = dict_tags[tag] + 1\n",
    "                else :\n",
    "                    dict_tags[tag] = 1\n",
    "            else:\n",
    "                dict_name[word] = 1\n",
    "                if tag in dict_tags :\n",
    "                    dict_tags[tag] = dict_tags[tag] + 1\n",
    "                else :\n",
    "                    dict_tags[tag] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIuLvkG08LkA",
    "outputId": "fba4840c-c6e7-4b1d-8755-220df72885b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr3xTl9F8LkA"
   },
   "outputs": [],
   "source": [
    "# Processin the Mixed Corpus to Remove Problems related to space before punctuations\n",
    "\n",
    "\n",
    "with open(\"auto_improved.txt\",\"w+\", encoding=\"utf-8\") as f :\n",
    "    for i in range(len(autom)) :\n",
    "        if  autom[i] != '.' :\n",
    "            if autom[i] == '।' :\n",
    "                f.write(\" \" + str(autom[i]))\n",
    "            elif autom[i] == \",\":\n",
    "                f.write(\" \" + str(autom[i]))\n",
    "            elif autom[i] == \"/\" :\n",
    "                f.write(\" \" + str(autom[i]))\n",
    "            elif autom[i] == '\"' :\n",
    "                f.write(\" \" + str(autom[i]))\n",
    "            elif autom[i] == \"''\" :\n",
    "                f.write(\" \" + str(autom[i]))\n",
    "            else :\n",
    "                f.write(str(autom[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da9XzoaQ8LkA"
   },
   "outputs": [],
   "source": [
    "f = open(\"words_none.txt\", \"w+\", encoding = \"utf-8\")\n",
    "a = open(\"auto_improved.txt\",\"r\", encoding = \"utf-8\").read().split()\n",
    "#print(a)\n",
    "for word in a:\n",
    "    if word not in dict_name :\n",
    "        #print(word)\n",
    "        f.write(str(word) + \"\\n\")\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aoq-2icE8LkB"
   },
   "outputs": [],
   "source": [
    "# English Analysis to see Hindi Derived words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7A1LvEZ8LkB"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank, brown\n",
    "\n",
    "# Train data\n",
    "train_data = treebank.tagged_sents()\n",
    "#print (train_data)\n",
    "\n",
    "# Import HMM module\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Setup a trainer with default(None) values\n",
    "# And train with the data\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sriLptZ8LkC"
   },
   "outputs": [],
   "source": [
    "dict_name_english={}\n",
    "dict_tags_english={}\n",
    "with open(\"dictionary_english.txt\", \"w+\", encoding ='utf-8') as f:\n",
    "    for sentence in train_data:\n",
    "        for (word,tag) in sentence :\n",
    "            if word in dict_name_english:\n",
    "                dict_name_english[word] = dict_name_english[word] + 1\n",
    "                if tag in dict_tags_english :\n",
    "                    dict_tags_english[tag] = dict_tags_english[tag] + 1\n",
    "                else :\n",
    "                    dict_tags_english[tag] = 1\n",
    "            else:\n",
    "                dict_name_english[word] = 1\n",
    "                if tag in dict_tags_english :\n",
    "                    dict_tags_english[tag] = dict_tags_english[tag] + 1\n",
    "                else :\n",
    "                    dict_tags_english[tag] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZQMAXVc8LkC"
   },
   "outputs": [],
   "source": [
    "# Sorting words which could not be translated even using Google translate\n",
    "\n",
    "f = open(\"translated.txt\", \"r\", encoding = \"utf-8\").read().split()\n",
    "g = open(\"not_translated.txt\", \"w+\", encoding = \"utf-8\")\n",
    "l = open(\"yes_translated.txt\", \"w+\", encoding = \"utf-8\")\n",
    "for word in f:\n",
    "    if word in dict_name_english :\n",
    "        l.write(str(word)+\"\\n\")\n",
    "    else :\n",
    "        g.write(str(word)+ \"\\n\")\n",
    "\n",
    "g.close()\n",
    "l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BJpggmp8LkC",
    "outputId": "89377976-5021-44ee-836e-63a34646a0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6533\n"
     ]
    }
   ],
   "source": [
    "hybrid = {}\n",
    "f = open(\"words_none.txt\", \"r\", encoding = \"utf-8\").read().split()\n",
    "unk_dict = {word : word for word in f}\n",
    "print(len(unk_dict) )\n",
    "for word in unk_dict :\n",
    "    if word not in dict_name_english :\n",
    "        hybrid[word] = str(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf5sYD3_8LkC",
    "outputId": "263e95cc-a98a-48d1-9ff1-2f6aaf138e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6316\n"
     ]
    }
   ],
   "source": [
    "print(len(hybrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52dxmTaF8LkD"
   },
   "outputs": [],
   "source": [
    "g = open(\"not_translated.txt\", \"w+\", encoding = \"utf-8\")\n",
    "for word in hybrid :\n",
    "    g.write(str(word) + \"\\n\")\n",
    "\n",
    "g.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_7CB_XB8LkD",
    "outputId": "6c459263-9f47-4699-f735-4f6ab7217a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6316 6316\n"
     ]
    }
   ],
   "source": [
    "l = open('translated.txt', \"r\", encoding = \"utf-8\").readlines()\n",
    "h = list(hybrid)\n",
    "print(len(l), len(h))\n",
    "\n",
    "for i in range(len(h)) :\n",
    "    if h[i] in unk_dict :\n",
    "        unk_dict[h[i]] = l[i].rstrip()\n",
    "#list(hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u06HKrt8LkD"
   },
   "outputs": [],
   "source": [
    "#creating a corpus which is now treatable with My POS tagging in both languages\n",
    "\n",
    "f = open(\"auto_improved.txt\", \"r\", encoding = \"utf-8\").read().split()\n",
    "l = open(\"changed_corpus.txt\", \"w+\", encoding = \"utf-8\")\n",
    "\n",
    "for word in f :\n",
    "    if word in unk_dict:\n",
    "        l.write(str(unk_dict[word])+\" \")\n",
    "    elif word == \"।\" :\n",
    "        l.write(str(word) + \"\\n\")\n",
    "    else :\n",
    "        l.write(str(word) + \" \")\n",
    "\n",
    "l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eK_HCq4q8LkD",
    "outputId": "053295bd-b249-445b-c038-d3afb5deb98e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Festive', 'NN'),\n",
       " ('सीजन', 'NN'),\n",
       " ('शुरू', 'NVB'),\n",
       " ('होने', 'VNN'),\n",
       " ('के', 'PREP'),\n",
       " ('साथ', 'PREP'),\n",
       " ('ही', 'RP'),\n",
       " ('Cars', 'NN'),\n",
       " ('के', 'PREP'),\n",
       " ('sales', 'NNS'),\n",
       " ('में', 'PREP'),\n",
       " ('Speed', 'NN'),\n",
       " ('आ', 'VFM'),\n",
       " ('जाती', 'VAUX'),\n",
       " ('है', 'VAUX')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS tagging model\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "e1 = nltk.UnigramTagger(train_data , backoff=t0)\n",
    "e2 = nltk.BigramTagger(train_data, backoff = e1)\n",
    "t1 = nltk.UnigramTagger(brown_tagged, backoff=e2)\n",
    "t2 = nltk.BigramTagger(brown_tagged, backoff=t1)\n",
    "tagger = t2.tag(\"top models में Maruti Suzuki की Cars हैं ।\".split())\n",
    "tagger\n",
    "sent2 = \"Festive सीजन शुरू होने के साथ ही Cars के sales में Speed आ जाती है \"\n",
    "tagger = t2.tag(sent2.split())\n",
    "tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QjPaKw18LkD",
    "outputId": "287a718e-23e4-4fce-898a-68e0eb73f2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP\n",
      "    (NP (NP (JJ top) (N models)) (PP (P में) (NP (N Maruti))))\n",
      "    (PP (P की) (NP (N Cars))))\n",
      "  (VP (V हैं))\n",
      "  ।)\n",
      "(S\n",
      "  (NP\n",
      "    (NP (JJ top) (N models))\n",
      "    (PP (P में) (NP (NP (N Maruti)) (PP (P की) (NP (N Cars))))))\n",
      "  (VP (V हैं))\n",
      "  ।)\n",
      "(S\n",
      "  (NP (JJ Festive) (N सीजन))\n",
      "  (VP\n",
      "    (Vh शुरू)\n",
      "    (VP\n",
      "      (Vh\n",
      "        (Vh\n",
      "          (Vh\n",
      "            (Vh होने)\n",
      "            (PP (P के) (PP (P साथ) (PP (P ही) (NP (N Cars))))))\n",
      "          (PP (P के) (NP (N sales))))\n",
      "        (PP (P में) (NP (N Speed))))\n",
      "      (VP (Vh आ) (VP (Vh जाती) (VP (V है)))))))\n",
      "(S\n",
      "  (NP (JJ Festive) (N सीजन))\n",
      "  (VP\n",
      "    (Vh शुरू)\n",
      "    (VP\n",
      "      (Vh\n",
      "        (Vh\n",
      "          (Vh होने)\n",
      "          (PP\n",
      "            (P के)\n",
      "            (PP\n",
      "              (P साथ)\n",
      "              (PP\n",
      "                (P ही)\n",
      "                (NP (NP (N Cars)) (PP (P के) (NP (N sales))))))))\n",
      "        (PP (P में) (NP (N Speed))))\n",
      "      (VP (Vh आ) (VP (Vh जाती) (VP (V है)))))))\n",
      "(S\n",
      "  (NP (JJ Festive) (N सीजन))\n",
      "  (VP\n",
      "    (Vh शुरू)\n",
      "    (VP\n",
      "      (Vh\n",
      "        (Vh होने)\n",
      "        (PP\n",
      "          (P के)\n",
      "          (PP\n",
      "            (P साथ)\n",
      "            (PP\n",
      "              (P ही)\n",
      "              (NP\n",
      "                (NP (NP (N Cars)) (PP (P के) (NP (N sales))))\n",
      "                (PP (P में) (NP (N Speed))))))))\n",
      "      (VP (Vh आ) (VP (Vh जाती) (VP (V है)))))))\n",
      "(S\n",
      "  (NP (JJ Festive) (N सीजन))\n",
      "  (VP\n",
      "    (Vh शुरू)\n",
      "    (VP\n",
      "      (Vh\n",
      "        (Vh होने)\n",
      "        (PP\n",
      "          (P के)\n",
      "          (PP\n",
      "            (P साथ)\n",
      "            (PP\n",
      "              (P ही)\n",
      "              (NP\n",
      "                (NP (N Cars))\n",
      "                (PP\n",
      "                  (P के)\n",
      "                  (NP (NP (N sales)) (PP (P में) (NP (N Speed))))))))))\n",
      "      (VP (Vh आ) (VP (Vh जाती) (VP (V है)))))))\n",
      "(S\n",
      "  (NP (JJ Festive) (N सीजन))\n",
      "  (VP\n",
      "    (Vh शुरू)\n",
      "    (VP\n",
      "      (Vh\n",
      "        (Vh\n",
      "          (Vh होने)\n",
      "          (PP (P के) (PP (P साथ) (PP (P ही) (NP (N Cars))))))\n",
      "        (PP (P के) (NP (NP (N sales)) (PP (P में) (NP (N Speed))))))\n",
      "      (VP (Vh आ) (VP (Vh जाती) (VP (V है)))))))\n",
      "(S (NP (JJ Festive) (N सीजन)) (VP (Vh शुरू) (VP (V है))))\n"
     ]
    }
   ],
   "source": [
    "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP \"।\" | NP VP\n",
    "  VP -> V NP | V NP PP | V | Vh VP\n",
    "  PP -> P NP | P PP | PP VP\n",
    "  V -> \"selling\" | \"हैं\" | \"है\" | \"था\"\n",
    "  NP -> Det N | Det N PP | N | JJ N | NP PP | N NP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  JJ -> \"top\" | \"Festive\" | \"west\"\n",
    "  N -> \"models\" | \"Cars\" | \"Maruti\" | \"सीजन\" | \"sales\" | \"Speed\" | \"beginning\" | \"Nano\" | \"बंगाल\" |\"Plant\"| \"Singur\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\" | \"में\" | \"की\" | \"साथ\" | \"ही\" | \"के\" | \"का\"\n",
    "  Vh -> \"जाती\" | \"शुरू\" | \"आ\" | \"होने\" | Vh PP | \"लगाया\" | P Vh\n",
    "  \"\"\")\n",
    "\n",
    "\n",
    "sent1 = \"top models में Maruti की Cars हैं ।\"\n",
    "sent2 = \"Festive सीजन शुरू होने के साथ ही Cars के sales में Speed आ जाती है \"\n",
    "sent3 = \"Festive सीजन शुरू है \"\n",
    "sent4 = \"beginning में Nano का Plant west बंगाल के Singur में लगाया था \"\n",
    "tokens = sent1.split()\n",
    "parser = nltk.parse.BottomUpLeftCornerChartParser(grammar2)\n",
    "tokens2 = sent2.split()\n",
    "tokens3 = sent3.split()\n",
    "for tree in parser.parse(tokens):\n",
    "    print(tree)\n",
    "    tree.draw()\n",
    "\n",
    "for tree in parser.parse(tokens2):\n",
    "    print(tree)\n",
    "    tree.draw()\n",
    "\n",
    "for tree in parser.parse(tokens3):\n",
    "    print(tree)\n",
    "    tree.draw()\n",
    "\n",
    "\n",
    "t2.tag(sent4.split())\n",
    "for tree in parser.parse(sent4.split()):\n",
    "    print(tree)\n",
    "    tree.draw()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_Mrl5Hv8LkE",
    "outputId": "b20a8cab-d1a7-4ffc-f1b8-b68d5594d69d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 219), ('NNP', 216), ('CD', 176), ('JJ', 78), ('NNS', 56), ('VB', 41), ('IN', 26), ('RB', 21), ('DT', 14), ('VBG', 14), ('VBP', 13), ('NNPS', 12), ('PRP$', 8), ('PRP', 7), ('VBN', 6), ('WRB', 5), ('VBD', 5), ('CC', 4), ('SYM', 3), ('VBZ', 3), ('PUNC', 2), ('JJR', 2), ('TO', 2), ('RP', 2), ('JJS', 2), ('WP', 2), ('EX', 1), ('WDT', 1), ('-NONE-', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Now lets find the Highest appearence of POS tags in words\n",
    "l  = open(\"changed_corpus.txt\", \"r\", encoding = \"utf-8\").read().split()\n",
    "dicte= {}\n",
    "for word in l:\n",
    "    if word in dict_name_english :\n",
    "        dicte[word] = 0\n",
    "l = dicte.items()\n",
    "a = [word for (word,tag) in l ]\n",
    "tags_for_words = t2.tag(a)\n",
    "\n",
    "dict_for_count = {}\n",
    "for (word,tag) in tags_for_words :\n",
    "    if tag in dict_for_count :\n",
    "        dict_for_count[tag] = dict_for_count[tag] + 1\n",
    "    else :\n",
    "        dict_for_count[tag] = 1\n",
    "\n",
    "d = dict_for_count\n",
    "q = [(tag,count) for (tag,count) in d.items()]\n",
    "\n",
    "def sortSecond(val):\n",
    "    return val[1]\n",
    "q.sort(key = sortSecond , reverse = True)\n",
    "print(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NEuz61q8LkE",
    "outputId": "1d216b1e-c368-471d-9de1-805d5e16ae17",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 234), ('VFM', 103), ('JJ', 71), ('PRP', 53), ('NNC', 44), ('VAUX', 41), ('NVB', 40), ('NNP', 37), ('VNN', 33), ('PREP', 30), ('JVB', 29), ('QFNUM', 24), ('NNPC', 19), ('RB', 18), ('CC', 13), ('NLOC', 10), ('INTF', 10), ('QF', 7), ('VRB', 6), ('VJJ', 5), ('SYM', 5), ('RP', 4), ('NEG', 4), ('PUNC', 3), ('QW', 3), ('', 1)]\n"
     ]
    }
   ],
   "source": [
    "#Now lets find the frequency of Hindi POS Tags in the corpus\n",
    "\n",
    "\n",
    "l  = open(\"changed_corpus.txt\", \"r\", encoding = \"utf-8\").read().split()\n",
    "dictef= {}\n",
    "for word in l:\n",
    "    if word in dict_name :\n",
    "        dictef[word] = 0\n",
    "l = dictef.items()\n",
    "a = [word for (word,tag) in l ]\n",
    "tags_for_words = t2.tag(a)\n",
    "\n",
    "#print(tags_for_words)\n",
    "dict_for_count = {}\n",
    "#print(dict_for_count)\n",
    "for (word,tag) in tags_for_words :\n",
    "    if tag in dict_for_count :\n",
    "        dict_for_count[tag] = dict_for_count[tag] + 1\n",
    "    else :\n",
    "        dict_for_count[tag] = 1\n",
    "\n",
    "d = dict_for_count\n",
    "\n",
    "g = [(tag,count) for (tag,count) in d.items()]\n",
    "#print(g)\n",
    "\n",
    "g.sort(key = sortSecond , reverse = True)\n",
    "print(g)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
